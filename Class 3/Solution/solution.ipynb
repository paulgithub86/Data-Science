# Fix the error by filling y with its mean for the CV runs (matches the "bad" baseline handling).
# Re-generate and overwrite the answer key file.

import numpy as np, pandas as pd
from math import sqrt
from datetime import datetime, timedelta

lines = []

def add(title): lines.append(f"\n## {title}\n")
def p(s=""): lines.append(s)

# (Re-run the same computations as before; only the final notebook 10 section is changed to fill y.)

# 01
add("01 — Stats Basics")
x = [10, 22, 30, 17, 18, 20, 30, 42, 32]
mean = lambda xs: sum(xs)/len(xs)
svar = lambda xs: sum((xi-mean(xs))**2 for xi in xs)/(len(xs)-1)
ssd = lambda xs: sqrt(svar(xs))
def mad(xs):
    med = np.median(xs)
    return float(np.median(np.abs(np.array(xs)-med)))
m = mean(x); v = svar(x); sd = ssd(x)
x_out = [10,22,30,17,18,20,30,420,32]
p(f"- Original: mean={m:.2f}, var={v:.2f}, sd={sd:.2f}")
p(f"- With outlier: mean={mean(x_out):.2f}, var={svar(x_out):.2f}, sd={ssd(x_out):.2f}")
p(f"- MAD(original)={mad(x):.2f}, MAD(outlier)={mad(x_out):.2f}")
p("- SD blows up with outlier; MAD stable.")

# 02
add("02 — Missingness Mechanisms")
np.random.seed(42)
df = pd.DataFrame({
    "ID": range(1,11),
    "Age":  [22,25,29,35,40,45,50,54,60,65],
    "Educ": [12,12,14,16,16,18,16,14,12,12],
    "Income":[25,28,32,40,48,55,62,60,50,45]
})
truth_mean = df["Income"].mean()

def run_mcar(df, p):
    nrun=200; means=[]
    for _ in range(nrun):
        m = df.copy()
        mask = np.random.rand(len(m)) < p
        m.loc[mask,"Income"] = np.nan
        cc = m.dropna(subset=["Income"])
        means.append(cc["Income"].mean())
    arr=np.array(means)
    return arr.mean()-truth_mean, arr.std()

for pval in [0.1,0.3,0.6]:
    bias,std = run_mcar(df,pval)
    p(f"- MCAR p={pval}: bias≈{bias:.2f}, run-SD≈{std:.2f}")

missing_ids=[1,2,3,8]
df_mar = df.copy()
df_mar["Income_missing"]=0
df_mar.loc[df_mar["ID"].isin(missing_ids),"Income"]=np.nan
df_mar.loc[df_mar["ID"].isin(missing_ids),"Income_missing"]=1
corr = df_mar["Income_missing"].corr(df_mar["Age"])
cc_mean = df_mar["Income"].dropna().mean()
p(f"- MAR: corr(missing,Age)={corr:.3f}; CC mean={cc_mean:.1f} vs truth {truth_mean:.1f} (biased high).")

# MNAR Δ
deltas=[0,-2,-5,-8]
imputed_low = np.array([24.4,27.1,32.3])
base_known = np.array([40,48,55,62,50,45])
means = [ float(np.concatenate([imputed_low+d, base_known]).mean()) for d in deltas ]
p(f"- Δ-sweep means≈ { [round(m,1) for m in means] }")

# 03
add("03 — Simple Imputation")
true_mean = df["Income"].mean()
df_mar = df.copy()
df_mar.loc[df_mar["ID"].isin(missing_ids),"Income"]=np.nan
cc_mean = df_mar["Income"].dropna().mean()
mean_val = cc_mean
imp_mean = df_mar["Income"].fillna(mean_val)
imp_grp = df_mar.copy()
grp_meds = imp_grp.groupby("Educ")["Income"].transform("median")
imp_grp["Income"] = imp_grp["Income"].fillna(grp_meds)
p(f"- CC mean={cc_mean:.1f}; mean-impute→ mean={imp_mean.mean():.1f}, sd={imp_mean.std(ddof=1):.2f}; group-median→ mean={imp_grp['Income'].mean():.1f}, sd={imp_grp['Income'].std(ddof=1):.2f}.")
p("- Indicator often carries signal under MAR; include it in downstream model.")

# 04
add("04 — kNN Imputer")
from sklearn.impute import KNNImputer
from sklearn.metrics import mean_absolute_error
def knn_mae(k, normalize=False, add_age2=False):
    d = df.copy(); truth = d.copy()
    d.loc[[0,1,2,7],"Income"]=np.nan  # IDs 1,2,3,8 -> 0-based
    Mat = d[["Age","Educ","Income"]].astype(float)
    if add_age2: Mat["Age2"]=Mat["Age"]**2
    if normalize:
        for c in Mat.columns:
            if c!="Income":
                v = Mat[c].values; Mat[c] = (v - v.mean())/(v.std() if v.std()!=0 else 1.0)
    Xi = KNNImputer(n_neighbors=k, weights="distance").fit_transform(Mat.values)
    inc = Xi[:,2]
    mask = d["Income"].isna().values
    return mean_absolute_error(truth.loc[mask,"Income"], inc[mask])
maes = {k: round(knn_mae(k),2) for k in [1,3,5,7]}
maes_scaled = {k: round(knn_mae(k, normalize=True),2) for k in [1,3,5,7]}
maes_age2 = {k: round(knn_mae(k, normalize=True, add_age2=True),2) for k in [1,3,5,7]}
p(f"- MAE (no scaling): {maes}")
p(f"- With scaling: {maes_scaled}")
p(f"- With Age^2 (scaled): {maes_age2}")

# 05
add("05 — Regression Imputation + Stochastic Residuals")
from sklearn.linear_model import LinearRegression
np.random.seed(13)
d = df.copy(); truth = d.copy()
d.loc[[0,1,2,7],"Income"]=np.nan
train = d.dropna(subset=["Income"])
Xtr, ytr = train[["Age","Educ"]].values, train["Income"].values
lr = LinearRegression().fit(Xtr,ytr)
pred_all = lr.predict(d[["Age","Educ"]].values)
resid = ytr - lr.predict(Xtr)
sigma = np.std(resid, ddof=1)
np.random.seed(999)
means=[]; variances=[]
for _ in range(50):
    inc_imp = d["Income"].copy()
    for i,isna in enumerate(d["Income"].isna().values):
        if isna:
            inc_imp.iat[i] = pred_all[i] + np.random.normal(0, sigma)
    means.append(float(np.mean(inc_imp)))
    variances.append(float(np.var(inc_imp, ddof=1)))
p(f"- 50-run pooled mean≈{np.mean(means):.2f}; pooled SD≈{sqrt(np.mean(variances)):.2f} (variance preserved).")

# 06
add("06 — MICE (IterativeImputer)")
from sklearn.experimental import enable_iterative_imputer  # noqa
from sklearn.impute import IterativeImputer
np.random.seed(7)
d = df.copy(); d.loc[[0,1,2,7],"Income"]=np.nan
truth_mean = df["Income"].mean()
def run_mice(M=5, sample_posterior=True, add_age2=False):
    means=[]; variances=[]
    for m in range(M):
        imp = IterativeImputer(random_state=100+m, sample_posterior=sample_posterior, max_iter=10)
        use = d[["Age","Educ","Income"]].copy()
        if add_age2: use["Age2"]=use["Age"]**2
        Xi = imp.fit_transform(use.values)
        inc = Xi[:,2]
        means.append(inc.mean())
        variances.append(inc.var(ddof=1))
    pooled_mean = float(np.mean(means))
    W = float(np.mean(variances))
    B = float(np.var(means, ddof=1)) if M>1 else 0.0
    T = float(W + (1+1/M)*B) if M>1 else W
    return pooled_mean, W, B, T
pm5 = run_mice(M=5, sample_posterior=True)
pm10 = run_mice(M=10, sample_posterior=True)
det5 = run_mice(M=5, sample_posterior=False)
age2 = run_mice(M=5, sample_posterior=True, add_age2=True)
p(f"- Truth mean={truth_mean:.2f}")
p(f"- M=5: pooled_mean={pm5[0]:.2f}, Within={pm5[1]:.2f}, Between={pm5[2]:.4f}, Total={pm5[3]:.2f}")
p(f"- M=10: pooled_mean={pm10[0]:.2f}, Within={pm10[1]:.2f}, Between={pm10[2]:.4f}, Total={pm10[3]:.2f}")
p(f"- sample_posterior=False: Between→{det5[2]:.4f} (collapse)")
p(f"- Add Age^2: pooled_mean={age2[0]:.2f} (small shift expected)")

# 07
add("07 — Outliers")
A = np.array([25,30,35,40,45,50,55,60,65])
B = np.array([25,30,35,40,45,50,55,60,65,500])
q1,q3 = np.percentile(B, [25,75])
iqr = q3-q1; lo,hi = q1-1.5*iqr, q3+1.5*iqr
B_win = np.clip(B, lo, hi)
meanB, sdB = float(np.mean(B)), float(np.std(B, ddof=1))
meanW, sdW = float(np.mean(B_win)), float(np.std(B_win, ddof=1))
def mad_np(arr):
    med = np.median(arr); return float(np.median(np.abs(arr-med)))
def rZ(arr):
    M = mad_np(arr); med = np.median(arr); M = M if M!=0 else 1.0
    return 0.6745*(arr - med)/M
p(f"- IQR fences: lo={lo:.2f}, hi={hi:.2f}; flagged={(B<lo)|(B>hi).sum()}.")
p(f"- Mean/SD before={meanB:.2f}/{sdB:.2f}; after winsorize={meanW:.2f}/{sdW:.2f}.")
p(f"- Robust-Z(500)≈{rZ(B)[-1]:.2f} (>3).")

# 08
add("08 — Tidy Reshape")
wide = pd.DataFrame({"id":["s1","s2","s3"],"math":[80,90,None],"phys":[70,88,75],"chem":[85,82,78]})
long = wide.melt(id_vars=["id"], var_name="subject", value_name="score")
meds = long.groupby("subject")["score"].transform("median")
long_imp = long.copy(); long_imp["score"]=long_imp["score"].fillna(meds)
wide_back = long_imp.pivot(index="id", columns="subject", values="score").reset_index()
p(f"- Subject medians: math={long[long.subject=='math']['score'].median()}, phys={long[long.subject=='phys']['score'].median()}, chem={long[long.subject=='chem']['score'].median()}")
p(f"- Wide after impute:\n{wide_back.to_string(index=False)}")
p("- Tidy rule satisfied in long form.")

# 09
add("09 — Data Quality Audit")
np.random.seed(123)
n=30; today = datetime(2025,9,24)
dfq = pd.DataFrame({
    "id":[f"u{i:03d}" for i in range(n)],
    "age": np.random.randint(-5,130,size=n),
    "email":[f"user{i}@example.com" for i in range(n)],
    "region": np.random.choice(["North","South","East","West"], size=n),
    "income": np.random.choice([np.nan,20,25,30,40,50,60,80], size=n, p=[0.2,0.1,0.1,0.15,0.15,0.15,0.1,0.05]),
    "updated_at":[today - timedelta(days=int(d)) for d in np.random.randint(0,60,size=n)],
})
dfq.loc[5,"email"]="bademail.example.com"
dfq = pd.concat([dfq, dfq.iloc[[3]]], ignore_index=True)
dfq.loc[10,"id"]=dfq.loc[3,"id"]
mask_age_bad = ~dfq["age"].between(0,110)
mask_email_bad = ~dfq["email"].astype(str).str.contains("@", regex=False)
mask_income_na = dfq["income"].isna()
mask_id_dup = dfq["id"].duplicated(keep=False)
mask_stale = (today - dfq["updated_at"]).dt.days > 30
counts = dict(age_bad=int(mask_age_bad.sum()), email_bad=int(mask_email_bad.sum()),
              income_na=int(mask_income_na.sum()), id_dups=int(mask_id_dup.sum()), stale=int(mask_stale.sum()))
p(f"- Issue counts: {counts}")
dfq_sorted = dfq.sort_values(["id","updated_at"], ascending=[True, False])
dedup = dfq_sorted.drop_duplicates(subset=["id"], keep="first")
p(f"- After dedup by latest: unique ids={dedup['id'].nunique()} rows={len(dedup)}")

# 10
add("10 — Pipeline (No Leakage)")
from sklearn.model_selection import KFold, cross_val_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import Ridge
from sklearn.ensemble import RandomForestRegressor

np.random.seed(2025)
n=120
age = np.random.randint(20,66,size=n)
educ = np.random.choice([12,14,16,18], size=n, p=[0.3,0.3,0.3,0.1])
region = np.random.choice(["North","South","East","West"], size=n)
income = 10 + 0.9*age + 0.8*(educ-12) + np.where(region=="North",5,0) + np.random.normal(0,6,size=n)
dfm = pd.DataFrame({"Age":age,"Educ":educ,"Region":region,"Income":income})
mask_mar = ((dfm["Age"]<28) | (dfm["Region"]=="South")) & (np.random.rand(n) < 0.35)
dfm.loc[mask_mar,"Income"] = np.nan
X = dfm[["Age","Educ","Region"]]; y = dfm["Income"]
y_filled = y.fillna(y.mean())

# BAD
X_bad = X.copy()
rng = np.random.default_rng(0)
X_bad.loc[:,["Age","Educ"]] = X_bad[["Age","Educ"]].mask(rng.random((n,2))<0.1)
global_imputer = SimpleImputer(strategy="median").fit(X_bad[["Age","Educ"]])
X_bad[["Age","Educ"]] = global_imputer.transform(X_bad[["Age","Educ"]])
X_bad = pd.get_dummies(X_bad, columns=["Region"], drop_first=True)
kf = KFold(n_splits=5, shuffle=True, random_state=42)
bad_score = cross_val_score(Ridge(alpha=1.0), X_bad, y_filled, cv=kf, scoring="neg_mean_absolute_error").mean()

# GOOD
num_pipe = Pipeline([("imputer", SimpleImputer(strategy="median")),("scaler", StandardScaler())])
cat_pipe = Pipeline([("encoder", OneHotEncoder(handle_unknown="ignore", drop="first"))])
ct = ColumnTransformer([("num", num_pipe, ["Age","Educ"]), ("cat", cat_pipe, ["Region"])])
pipe = Pipeline([("prep", ct), ("model", Ridge(alpha=1.0))])
good_score = cross_val_score(pipe, X, y_filled, cv=kf, scoring="neg_mean_absolute_error").mean()

pipe_rf = Pipeline([("prep", ct), ("model", RandomForestRegressor(n_estimators=200, random_state=0))])
good_score_rf = cross_val_score(pipe_rf, X, y_filled, cv=kf, scoring="neg_mean_absolute_error").mean()

p(f"- Leaky Ridge (neg MAE): {bad_score:.3f}")
p(f"- Pipeline Ridge (neg MAE): {good_score:.3f}")
p(f"- Pipeline RF (neg MAE): {good_score_rf:.3f}")
p("- Expect: leaky looks better than it should; pipeline is trustworthy.")

# Write file
out = "/mnt/data/Answer_Key_CS531_10_Notebooks.md"
with open(out, "w", encoding="utf-8") as f:
    f.write("# CS531 Coding Practice — Answer Key\n")
    f.write("\n".join(lines))
out
